{"cells":[{"cell_type":"markdown","metadata":{},"source":["# English"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import openai\n","import json\n","import os\n","import re\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Retrieve the OpenAI API key from the environment variable\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# Define the prompting method and language family\n","language_family = \"english\"\n","method = \"direct_inference\"\n","model = \"gpt-4o\"\n","\n","# Load data\n","input_file_path = f'../../JSON Files/{language_family}.json'\n","output_file_path = f\"{language_family}_{model}_{method}.csv\"\n","summary_file_path = f\"{language_family}_{model}_{method}_summary.json\"\n","\n","if os.path.exists(input_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","else:\n","    raise FileNotFoundError(f\"File not found: {input_file_path}\")\n","\n","claims = [item['claim'] for item in data]\n","labels = [item['label'] for item in data]\n","languages = [item['language'] for item in data]\n","sites = [item['site'] for item in data]\n","\n","df = pd.DataFrame({\n","    'claim': claims,\n","    'label': labels,\n","    'language': languages,\n","    'site': sites\n","})\n","\n","def get_gpt_response(claim, model=model):\n","    user_prompt = f\"'{claim}' Is this claim 'True', 'Mostly True', 'Half True', 'Mostly False', or 'False'? You must always make sure your final response is prefixed with 'Final Answer:' followed by either 'True', 'Mostly True', 'Half True', 'Mostly False', or 'False'.\"\n","    \n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        messages=[\n","            {\"role\": \"user\", \"content\": user_prompt}\n","        ],\n","        temperature=0,\n","    )\n","    return response['choices'][0]['message']['content']\n","\n","# Initialize or load existing outputs and summary\n","if os.path.exists(output_file_path):\n","    output_df = pd.read_csv(output_file_path)\n","    outputs = output_df.to_dict('records')\n","else:\n","    outputs = []\n","\n","if os.path.exists(summary_file_path):\n","    with open(summary_file_path, 'r', encoding='utf-8') as file:\n","        summary = json.load(file)\n","else:\n","    summary = {\n","        'correct': 0,\n","        'wrong': 0,\n","        'inconclusive': 0,\n","        'total': 0,\n","        'languages': {}\n","    }\n","\n","# Function to clean the output text\n","def clean_output(output):\n","    # Remove non-alphanumeric characters except spaces\n","    cleaned_output = re.sub(r'[^a-zA-Z\\s]', '', output)\n","    return cleaned_output\n","\n","# Process claims and update files iteratively\n","for index, row in df.iterrows():\n","    if any(output['claim'] == row['claim'] for output in outputs):\n","        continue  # Skip already processed claims\n","\n","    claim = row['claim']\n","    label = row['label']\n","    language = row['language']\n","    output = get_gpt_response(claim)\n","    \n","    # Print the model's output\n","    print(f\"Model Output: {output}\")\n","    \n","    # Clean the output\n","    cleaned_output = clean_output(output)\n","    \n","    # Extract final answer from the cleaned output\n","    final_answer = None\n","    if \"final answer true\" in cleaned_output.lower():\n","        final_answer = \"true\"\n","    elif \"final answer false\" in cleaned_output.lower():\n","        final_answer = \"false\"\n","    elif \"final answer mostly true\" in cleaned_output.lower():\n","        final_answer = \"mostly true\"\n","    elif \"final answer mostly false\" in cleaned_output.lower():\n","        final_answer = \"mostly false\"\n","    elif \"final answer half true\" in cleaned_output.lower():\n","        final_answer = \"half true\"\n","    \n","    # Determine correctness or inconclusiveness\n","    if final_answer is None:\n","        print(\"Inconclusive response\")\n","        summary['inconclusive'] += 1\n","    else:\n","        print(f\"Final Answer: {final_answer.capitalize()}, Actual Answer: {label.capitalize()}\")\n","        if final_answer == label.lower():\n","            print(\"Correct response\")\n","            summary['correct'] += 1\n","        else:\n","            print(\"Wrong response\")\n","            summary['wrong'] += 1\n","    \n","    # Save outputs\n","    output_record = {\n","        'claim': claim,\n","        'label': label,\n","        'language': language,\n","        'output': output,\n","        'final_answer': final_answer,\n","        'correct': final_answer == label.lower() if final_answer else False,\n","        'inconclusive': final_answer is None\n","    }\n","    outputs.append(output_record)\n","    \n","    # Update language summary\n","    if language not in summary['languages']:\n","        summary['languages'][language] = {'correct': 0, 'wrong': 0, 'inconclusive': 0, 'total': 0}\n","    summary['languages'][language]['total'] += 1\n","    summary['total'] += 1\n","    if final_answer is None:\n","        summary['languages'][language]['inconclusive'] += 1\n","    elif final_answer == label.lower():\n","        summary['languages'][language]['correct'] += 1\n","    else:\n","        summary['languages'][language]['wrong'] += 1\n","\n","    # Save results to CSV iteratively\n","    pd.DataFrame(outputs).to_csv(output_file_path, index=False, encoding='utf-8')\n","\n","    # Save summary to JSON iteratively\n","    with open(summary_file_path, 'w', encoding='utf-8') as file:\n","        json.dump(summary, file, ensure_ascii=False, indent=4)\n","\n","print(f\"Results saved to {output_file_path} and {summary_file_path}\")\n","print(f\"Summary: {summary}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Romance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import openai\n","import json\n","import os\n","import re\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Retrieve the OpenAI API key from the environment variable\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# Define the prompting method and language family\n","language_family = \"romance\"\n","method = \"direct_inference\"\n","model = \"gpt-4o\"\n","\n","# Load data\n","input_file_path = f'../../JSON Files/{language_family}.json'\n","output_file_path = f\"{language_family}_{model}_{method}.csv\"\n","summary_file_path = f\"{language_family}_{model}_{method}_summary.json\"\n","\n","if os.path.exists(input_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","else:\n","    raise FileNotFoundError(f\"File not found: {input_file_path}\")\n","\n","claims = [item['claim'] for item in data]\n","labels = [item['label'] for item in data]\n","languages = [item['language'] for item in data]\n","sites = [item['site'] for item in data]\n","\n","df = pd.DataFrame({\n","    'claim': claims,\n","    'label': labels,\n","    'language': languages,\n","    'site': sites\n","})\n","\n","# Language-specific user prompts and final answer mappings\n","prompts = {\n","    \"es\": \"'{claim}' ¿Es esta afirmación «Verdadera», «Casi verdadera», «Medio verdadera», «Casi falsa» o «Falsa»? Debe asegurarse de que su respuesta final comience con «Respuesta final:» seguida de «Verdadero», «Casi verdadero», «Medio verdadero», «Casi falso» o «Falso».\",\n","    \"it\": \"'{claim}' Questa affermazione è “vera”, “quasi vera”, “mezza vera”, “quasi falsa” o “falsa”? Assicurati che la tua risposta finale inizi con “Risposta finale:” seguita da “Vero”, “Quasi vero”, “Mezzo vero”, “Quasi falso” o “Falso”.\",\n","    \"pt\": \"'{claim}' Esta afirmação é “Verdadeira”, “Quase Verdadeira”, “Meia Verdadeira”, “Quase Falsa” ou “Falsa”? Certifique-se de que sua resposta final comece com “Resposta Final:” seguida de “Verdadeiro”, “Quase Verdadeiro”, “Meio Verdadeiro”, “Quase Falso” ou “Falso”.\",\n","    \"fr\": \"'{claim}' Cette affirmation est-elle « vraie », « en grande partie vraie », « à moitié vraie », « en grande partie fausse » ou « fausse » ? Vous devez vous assurer que votre réponse finale commence par « Réponse finale: » suivie de « Vrai », « En grande partie vrai », « À moitié vrai », « En grande partie faux » ou « Faux ».\"\n","}\n","\n","final_answer_mappings = {\n","    \"es\": {\n","        \"respuesta final: verdadero\": \"true\", \"respuesta final: verdadera\": \"true\",\n","        \"respuesta final: casi verdadero\": \"mostly true\", \"respuesta final: casi verdadera\": \"mostly true\",\n","        \"respuesta final: medio verdadero\": \"half true\", \"respuesta final: medio verdadera\": \"half true\",\n","        \"respuesta final: casi falso\": \"mostly false\", \"respuesta final: casi falsa\": \"mostly false\",\n","        \"respuesta final: falso\": \"false\", \"respuesta final: falsa\": \"false\",\n","        \"verdadero\": \"true\", \"verdadera\": \"true\",\n","        \"casi verdadero\": \"mostly true\", \"casi verdadera\": \"mostly true\",\n","        \"medio verdadero\": \"half true\", \"medio verdadera\": \"half true\",\n","        \"casi falso\": \"mostly false\", \"casi falsa\": \"mostly false\",\n","        \"falso\": \"false\", \"falsa\": \"false\"\n","    },\n","    \"it\": {\n","        \"risposta finale: vero\": \"true\", \"risposta finale: vera\": \"true\",\n","        \"risposta finale: quasi vero\": \"mostly true\", \"risposta finale: quasi vera\": \"mostly true\",\n","        \"risposta finale: mezzo vero\": \"half true\", \"risposta finale: mezza vera\": \"half true\",\n","        \"risposta finale: quasi falso\": \"mostly false\", \"risposta finale: quasi falsa\": \"mostly false\",\n","        \"risposta finale: falso\": \"false\", \"risposta finale: falsa\": \"false\",\n","        \"vero\": \"true\", \"vera\": \"true\",\n","        \"quasi vero\": \"mostly true\", \"quasi vera\": \"mostly true\",\n","        \"mezzo vero\": \"half true\", \"mezza vera\": \"half true\",\n","        \"quasi falso\": \"mostly false\", \"quasi falsa\": \"mostly false\",\n","        \"falso\": \"false\", \"falsa\": \"false\"\n","    },\n","    \"pt\": {\n","        \"resposta final: verdadeiro\": \"true\", \"resposta final: verdadeira\": \"true\",\n","        \"resposta final: quase verdadeiro\": \"mostly true\", \"resposta final: quase verdadeira\": \"mostly true\",\n","        \"resposta final: meio verdadeiro\": \"half true\", \"resposta final: meia verdadeira\": \"half true\",\n","        \"resposta final: quase falso\": \"mostly false\", \"resposta final: quase falsa\": \"mostly false\",\n","        \"resposta final: falso\": \"false\", \"resposta final: falsa\": \"false\",\n","        \"verdadeiro\": \"true\", \"verdadeira\": \"true\",\n","        \"quase verdadeiro\": \"mostly true\", \"quase verdadeira\": \"mostly true\",\n","        \"meio verdadeiro\": \"half true\", \"meia verdadeira\": \"half true\",\n","        \"quase falso\": \"mostly false\", \"quase falsa\": \"mostly false\",\n","        \"falso\": \"false\", \"falsa\": \"false\"\n","    },\n","    \"fr\": {\n","        \"réponse finale: vrai\": \"true\", \"réponse finale: vraie\": \"true\",\n","        \"réponse finale: plus ou moins vrai\": \"mostly true\", \"réponse finale: plus ou moins vraie\": \"mostly true\",\n","        \"réponse finale: moitié vrai\": \"half true\", \"réponse finale: moitié vraie\": \"half true\",\n","        \"réponse finale: plus ou moins faux\": \"mostly false\", \"réponse finale: plus ou moins fausse\": \"mostly false\",\n","        \"réponse finale: faux\": \"false\", \"réponse finale: fausse\": \"false\",\n","        \"vrai\": \"true\", \"vraie\": \"true\",\n","        \"plus ou moins vrai\": \"mostly true\", \"plus ou moins vraie\": \"mostly true\",\n","        \"moitié vrai\": \"half true\", \"moitié vraie\": \"half true\",\n","        \"plus ou moins faux\": \"mostly false\", \"plus ou moins fausse\": \"mostly false\",\n","        \"faux\": \"false\", \"fausse\": \"false\"\n","    }\n","}\n","\n","def get_gpt_response(claim, language, model=model):\n","    try:\n","        user_prompt = prompts[language].format(claim=claim)\n","        response = openai.ChatCompletion.create(\n","            model=model,\n","            messages=[\n","                {\"role\": \"user\", \"content\": user_prompt}\n","            ],\n","            temperature=0,\n","        )\n","        return response['choices'][0]['message']['content']\n","    except Exception as e:\n","        print(f\"Error getting GPT response: {e}\")\n","        return \"\"\n","\n","def clean_output(output):\n","    cleaned_output = re.sub(r'[^a-zA-Z\\s:]', '', output)\n","    return cleaned_output.lower()\n","\n","def extract_final_answer(cleaned_output, language):\n","    try:\n","        final_answer = re.search(r'(respuesta final:|risposta finale:|resposta final:|réponse finale:)\\s*([\\w\\s]+)', cleaned_output)\n","        if final_answer:\n","            response = final_answer.group(2).strip()\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in response:\n","                    return answer\n","        else:\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in cleaned_output:\n","                    return answer\n","    except Exception as e:\n","        print(f\"Error extracting final answer: {e}\")\n","    return None\n","\n","# Initialize or load existing outputs and summary\n","if os.path.exists(output_file_path):\n","    output_df = pd.read_csv(output_file_path)\n","    outputs = output_df.to_dict('records')\n","else:\n","    outputs = []\n","\n","if os.path.exists(summary_file_path):\n","    with open(summary_file_path, 'r', encoding='utf-8') as file:\n","        summary = json.load(file)\n","else:\n","    summary = {\n","        'correct': 0,\n","        'wrong': 0,\n","        'inconclusive': 0,\n","        'total': 0,\n","        'languages': {}\n","    }\n","\n","# Process claims and update files iteratively\n","for index, row in df.iterrows():\n","    if any(output['claim'] == row['claim'] for output in outputs):\n","        continue  # Skip already processed claims\n","\n","    claim = row['claim']\n","    label = row['label']\n","    language = row['language']\n","    \n","    if language not in prompts:  # Skip unsupported languages\n","        continue\n","\n","    output = get_gpt_response(claim, language)\n","    \n","    print(f\"Model Output: {output}\")\n","    \n","    cleaned_output = clean_output(output)\n","    final_answer = extract_final_answer(cleaned_output, language)\n","    \n","    if final_answer is None:\n","        print(\"Inconclusive response\")\n","        summary['inconclusive'] += 1\n","    else:\n","        print(f\"Final Answer: {final_answer.capitalize()}, Actual Answer: {label.capitalize()}\")\n","        if final_answer == label.lower():\n","            print(\"Correct response\")\n","            summary['correct'] += 1\n","        else:\n","            print(\"Wrong response\")\n","            summary['wrong'] += 1\n","    \n","    # Save outputs\n","    output_record = {\n","        'claim': claim,\n","        'label': label,\n","        'language': language,\n","        'output': output,\n","        'final_answer': final_answer,\n","        'correct': final_answer == label.lower() if final_answer else False,\n","        'inconclusive': final_answer is None\n","    }\n","    outputs.append(output_record)\n","    \n","    # Update language summary\n","    if language not in summary['languages']:\n","        summary['languages'][language] = {'correct': 0, 'wrong': 0, 'inconclusive': 0, 'total': 0}\n","    summary['languages'][language]['total'] += 1\n","    summary['total'] += 1\n","    if final_answer is None:\n","        summary['languages'][language]['inconclusive'] += 1\n","    elif final_answer == label.lower():\n","        summary['languages'][language]['correct'] += 1\n","    else:\n","        summary['languages'][language]['wrong'] += 1\n","\n","    # Save results to CSV iteratively\n","    pd.DataFrame(outputs).to_csv(output_file_path, index=False, encoding='utf-8')\n","\n","    # Save summary to JSON iteratively\n","    with open(summary_file_path, 'w', encoding='utf-8') as file:\n","        json.dump(summary, file, ensure_ascii=False, indent=4)\n","\n","print(f\"Results saved to {output_file_path} and {summary_file_path}\")\n","print(f\"Summary: {summary}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Indo-Aryan"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import openai\n","import json\n","import os\n","import re\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Retrieve the OpenAI API key from the environment variable\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# Define the prompting method and language family\n","language_family = \"indo_aryan\"\n","method = \"direct_inference\"\n","model = \"gpt-4o\"\n","\n","# Load data\n","input_file_path = f'../../JSON Files/{language_family}.json'\n","output_file_path = f\"{language_family}_{model}_{method}.csv\"\n","summary_file_path = f\"{language_family}_{model}_{method}_summary.json\"\n","\n","if os.path.exists(input_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","else:\n","    raise FileNotFoundError(f\"File not found: {input_file_path}\")\n","\n","claims = [item['claim'] for item in data]\n","labels = [item['label'] for item in data]\n","languages = [item['language'] for item in data]\n","sites = [item['site'] for item in data]\n","\n","df = pd.DataFrame({\n","    'claim': claims,\n","    'label': labels,\n","    'language': languages,\n","    'site': sites\n","})\n","\n","# Language-specific user prompts and final answer mappings\n","prompts = {\n","    \"hi\": \"'{claim}' क्या यह कथन «सत्य», «अधिकांशतः सत्य», «आधा सत्य», «अधिकांशतः असत्य» या «असत्य» है? आपको हमेशा यह सुनिश्चित करना चाहिए कि आपका अंतिम उत्तर «अंतिम उत्तर:» के साथ शुरू हो और उसके बाद «सत्य», «अधिकांशतः सत्य», «आधा सत्य», «अधिकांशतः असत्य» या «असत्य» हो।\",\n","    \"bn\": \"'{claim}' এই বক্তব্যটি «সত্য», «অধিকাংশ সত্য», «অর্ধসত্য», «অধিকাংশ মিথ্যা» বা «মিথ্যা» কি? সর্বদা নিশ্চিত করুন যে আপনার চূড়ান্ত উত্তর «চূড়ান্ত উত্তর:» দিয়ে শুরু হয় এবং তারপরে «সত্য», «অধিকাংশ সত্য», «অর্ধসত্য», «অধিকাংশ মিথ্যা» বা «মিথ্যা»।\",\n","    \"pa\": \"'{claim}' ਕੀ ਇਹ ਬਿਆਨ «ਸੱਚ», «ਜਿਆਦਾਤਰ ਸੱਚ», «ਅੱਧਾ ਸੱਚ», «ਜਿਆਦਾਤਰ ਝੂਠ» ਜਾਂ «ਝੂਠ» ਹੈ? ਤੁਹਾਨੂੰ ਹਮੇਸ਼ਾਂ ਇਹ ਯਕੀਨੀ ਬਣਾਉਣਾ ਚਾਹੀਦਾ ਹੈ ਕਿ ਤੁਹਾਡਾ ਅੰਤਿਮ ਜਵਾਬ «ਅੰਤਿਮ ਜਵਾਬ:» ਨਾਲ ਸ਼ੁਰੂ ਹੋਵੇ ਅਤੇ ਫਿਰ «ਸੱਚ», «ਜਿਆਦਾਤਰ ਸੱਚ», «ਅੱਧਾ ਸੱਚ», «ਜਿਆਦਾਤਰ ਝੂਠ» ਜਾਂ «ਝੂਠ» ਹੋਵੇ।\",\n","    \"gu\": \"'{claim}' શું આ નિવેદન «સત્ય», «મોટાભાગનું સત્ય», «અડધું સત્ય», «મોટાભાગનું ખોટું» કે «ખોટું» છે? તમે હંમેશાં ખાતરી કરવી જોઈએ કે તમારો અંતિમ જવાબ «અંતિમ જવાબ:» થી શરૂ થાય અને પછી «સત્ય», «મોટાભાગનું સત્ય», «અડધું સત્ય», «મોટાભાગનું ખોટું» કે «ખોટું» આવે।\",\n","    \"mr\": \"'{claim}' हे विधान «सत्य», «जास्तीत जास्त सत्य», «अर्धसत्य», «जास्तीत जास्त असत्य» किंवा «असत्य» आहे का? तुम्ही नेहमी हे सुनिश्चित केले पाहिजे की तुमचे अंतिम उत्तर «अंतिम उत्तर:» ने सुरू होते आणि त्यानंतर «सत्य», «जास्तीत जास्त सत्य», «अर्धसत्य», «जास्तीत जास्त असत्य» किंवा «असत्य» येते.\"\n","}\n","\n","final_answer_mappings = {\n","    \"hi\": {\n","        \"अंतिम उत्तर: सत्य\": \"true\",\n","        \"अंतिम उत्तर: अधिकांशतः सत्य\": \"mostly true\",\n","        \"अंतिम उत्तर: आधा सत्य\": \"half true\",\n","        \"अंतिम उत्तर: अधिकांशतः असत्य\": \"mostly false\",\n","        \"अंतिम उत्तर: असत्य\": \"false\",\n","        \"सत्य\": \"true\",\n","        \"अधिकांशतः सत्य\": \"mostly true\",\n","        \"आधा सत्य\": \"half true\",\n","        \"अधिकांशतः असत्य\": \"mostly false\",\n","        \"असत्य\": \"false\"\n","    },\n","    \"bn\": {\n","        \"চূড়ান্ত উত্তর: সত্য\": \"true\",\n","        \"চূড়ান্ত উত্তর: অধিকাংশ সত্য\": \"mostly true\",\n","        \"চূড়ান্ত উত্তর: অর্ধসত্য\": \"half true\",\n","        \"চূড়ান্ত উত্তর: অধিকাংশ মিথ্যা\": \"mostly false\",\n","        \"চূড়ান্ত উত্তর: মিথ্যা\": \"false\",\n","        \"সত্য\": \"true\",\n","        \"অধিকাংশ সত্য\": \"mostly true\",\n","        \"অর্ধসত্য\": \"half true\",\n","        \"অধিকাংশ মিথ্যা\": \"mostly false\",\n","        \"মিথ্যা\": \"false\"\n","    },\n","    \"pa\": {\n","        \"ਅੰਤਿਮ ਜਵਾਬ: ਸੱਚ\": \"true\",\n","        \"ਅੰਤਿਮ ਜਵਾਬ: ਜਿਆਦਾਤਰ ਸੱਚ\": \"mostly true\",\n","        \"ਅੰਤਿਮ ਜਵਾਬ: ਅੱਧਾ ਸੱਚ\": \"half true\",\n","        \"ਅੰਤਿਮ ਜਵਾਬ: ਜਿਆਦਾਤਰ ਝੂਠ\": \"mostly false\",\n","        \"ਅੰਤਿਮ ਜਵਾਬ: ਝੂਠ\": \"false\",\n","        \"ਸੱਚ\": \"true\",\n","        \"ਜਿਆਦਾਤਰ ਸੱਚ\": \"mostly true\",\n","        \"ਅੱਧਾ ਸੱਚ\": \"half true\",\n","        \"ਜਿਆਦਾਤਰ ਝੂਠ\": \"mostly false\",\n","        \"ਝੂਠ\": \"false\"\n","    },\n","    \"gu\": {\n","        \"અંતિમ જવાબ: સત્ય\": \"true\",\n","        \"અંતિમ જવાબ: મોટાભાગનું સત્ય\": \"mostly true\",\n","        \"અંતિમ જવાબ: અડધું સત્ય\": \"half true\",\n","        \"અંતિમ જવાબ: મોટાભાગનું ખોટું\": \"mostly false\",\n","        \"અંતિમ જવાબ: ખોટું\": \"false\",\n","        \"સત્ય\": \"true\",\n","        \"મોટાભાગનું સત્ય\": \"mostly true\",\n","        \"અડધું સત્ય\": \"half true\",\n","        \"મોટાભાગનું ખોટું\": \"mostly false\",\n","        \"ખોટું\": \"false\"\n","    },\n","    \"mr\": {\n","        \"अंतिम उत्तर: सत्य\": \"true\",\n","        \"अंतिम उत्तर: जास्तीत जास्त सत्य\": \"mostly true\",\n","        \"अंतिम उत्तर: अर्धसत्य\": \"half true\",\n","        \"अंतिम उत्तर: जास्तीत जास्त असत्य\": \"mostly false\",\n","        \"अंतिम उत्तर: असत्य\": \"false\",\n","        \"सत्य\": \"true\",\n","        \"जास्तीत जास्त सत्य\": \"mostly true\",\n","        \"अर्धसत्य\": \"half true\",\n","        \"जास्तीत जास्त असत्य\": \"mostly false\",\n","        \"असत्य\": \"false\"\n","    }\n","}\n","\n","def get_gpt_response(claim, language, model=model):\n","    try:\n","        user_prompt = prompts[language].format(claim=claim)\n","        response = openai.ChatCompletion.create(\n","            model=model,\n","            messages=[\n","                {\"role\": \"user\", \"content\": user_prompt}\n","            ],\n","            temperature=0,\n","        )\n","        return response['choices'][0]['message']['content']\n","    except Exception as e:\n","        print(f\"Error getting GPT response: {e}\")\n","        return \"\"\n","\n","def clean_output(output):\n","    cleaned_output = re.sub(r'[^a-zA-Z\\s:]', '', output)\n","    return cleaned_output.lower()\n","\n","def extract_final_answer(cleaned_output, language):\n","    try:\n","        final_answer = re.search(r'(अंतिम उत्तर:|चূড়ান্ত উত্তর:|ਅੰਤਿਮ ਜਵਾਬ:|અંતિમ જવાબ:|अंतिम उत्तर:)\\s*([\\w\\s]+)', cleaned_output)\n","        if final_answer:\n","            response = final_answer.group(2).strip()\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in response:\n","                    return answer\n","        else:\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in cleaned_output:\n","                    return answer\n","    except Exception as e:\n","        print(f\"Error extracting final answer: {e}\")\n","    return None\n","\n","# Initialize or load existing outputs and summary\n","if os.path.exists(output_file_path):\n","    output_df = pd.read_csv(output_file_path)\n","    outputs = output_df.to_dict('records')\n","else:\n","    outputs = []\n","\n","if os.path.exists(summary_file_path):\n","    with open(summary_file_path, 'r', encoding='utf-8') as file:\n","        summary = json.load(file)\n","else:\n","    summary = {\n","        'correct': 0,\n","        'wrong': 0,\n","        'inconclusive': 0,\n","        'total': 0,\n","        'languages': {}\n","    }\n","\n","# Process claims and update files iteratively\n","for index, row in df.iterrows():\n","    if any(output['claim'] == row['claim'] for output in outputs):\n","        continue  # Skip already processed claims\n","\n","    claim = row['claim']\n","    label = row['label']\n","    language = row['language']\n","    \n","    if language not in prompts:  # Skip unsupported languages\n","        continue\n","\n","    output = get_gpt_response(claim, language)\n","    \n","    print(f\"Model Output: {output}\")\n","    \n","    cleaned_output = clean_output(output)\n","    final_answer = extract_final_answer(cleaned_output, language)\n","    \n","    if final_answer is None:\n","        print(\"Inconclusive response\")\n","        summary['inconclusive'] += 1\n","    else:\n","        print(f\"Final Answer: {final_answer.capitalize()}, Actual Answer: {label.capitalize()}\")\n","        if final_answer == label.lower():\n","            print(\"Correct response\")\n","            summary['correct'] += 1\n","        else:\n","            print(\"Wrong response\")\n","            summary['wrong'] += 1\n","    \n","    # Save outputs\n","    output_record = {\n","        'claim': claim,\n","        'label': label,\n","        'language': language,\n","        'output': output,\n","        'final_answer': final_answer,\n","        'correct': final_answer == label.lower() if final_answer else False,\n","        'inconclusive': final_answer is None\n","    }\n","    outputs.append(output_record)\n","    \n","    # Update language summary\n","    if language not in summary['languages']:\n","        summary['languages'][language] = {'correct': 0, 'wrong': 0, 'inconclusive': 0, 'total': 0}\n","    summary['languages'][language]['total'] += 1\n","    summary['total'] += 1\n","    if final_answer is None:\n","        summary['languages'][language]['inconclusive'] += 1\n","    elif final_answer == label.lower():\n","        summary['languages'][language]['correct'] += 1\n","    else:\n","        summary['languages'][language]['wrong'] += 1\n","\n","    # Save results to CSV iteratively\n","    pd.DataFrame(outputs).to_csv(output_file_path, index=False, encoding='utf-8')\n","\n","    # Save summary to JSON iteratively\n","    with open(summary_file_path, 'w', encoding='utf-8') as file:\n","        json.dump(summary, file, ensure_ascii=False, indent=4)\n","\n","print(f\"Results saved to {output_file_path} and {summary_file_path}\")\n","print(f\"Summary: {summary}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Kartvelian"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Output: სოფლის მეურნეობაში პროგრესის შეფასება დამოკიდებულია სხვადასხვა ფაქტორებზე, როგორიცაა წარმოების ზრდა, ტექნოლოგიური ინოვაციები, გარემოს დაცვა და ეკონომიკური მაჩვენებლები. თუ ამ სფეროში არსებობს მონაცემები, რომლებიც აჩვენებს მნიშვნელოვან გაუმჯობესებას ამ მიმართულებებით, მაშინ პრეტენზია შეიძლება იყოს მართალი ან ძირითადად მართალი. თუმცა, თუ პროგრესი არ არის თანმიმდევრული ან არსებობს მნიშვნელოვანი პრობლემები, პრეტენზია შეიძლება იყოს ნახევრად მართალი ან მცდარი.\n","\n","საბოლოო პასუხი: ნახევრად მართალია.\n","Inconclusive response\n","Model Output: იმისათვის, რომ განვსაზღვროთ, არის თუ არა ეს პრეტენზია მართალი, საჭიროა განვიხილოთ რამდენიმე ფაქტორი:\n","\n","1. **წყარო და მონაცემები**: საჭიროა სანდო კვლევების მონაცემები, რომლებიც აჩვენებს პოლიციისადმი ნდობის დონეს წლების განმავლობაში.\n","2. **დროის პერიოდი**: უნდა განისაზღვროს, რომელ წლების განმავლობაში მოხდა ნდობის ცვლილება.\n","3. **გეოგრაფიული არეალი**: უნდა განისაზღვროს, რომელ ქვეყანაში ან რეგიონში მოხდა ეს ცვლილება.\n","\n","თუ არსებობს სანდო კვლევები, რომლებიც აჩვენებს, რომ პოლიციისადმი ნდობა პირველად ამ წლების განმავლობაში დაეცა, მაშინ პრეტენზია შეიძლება იყოს მართალი. თუმცა, თუ ასეთი მონაცემები არ არსებობს ან არ არის საკმარისი, პრეტენზია შეიძლება იყოს მცდარი.\n","\n","მიუხედავად ამისა, თუ არ გვაქვს კონკრეტული მონაცემები და კვლევები, ვერ შევძლებთ ზუსტად განვსაზღვროთ პრეტენზიის სისწორე.\n","\n","საბოლოო პასუხი: ნახევრად მართალია.\n","Inconclusive response\n","Model Output: საგარეო შოკების შერბილების მიზნით, მთავრობამ 2014 წლის ბიუჯეტის დეფიციტი შეამცირა. ამ პრეტენზიის სიზუსტის შესაფასებლად საჭიროა რამდენიმე ფაქტორის გათვალისწინება:\n","\n","1. **საგარეო შოკები**: ეს შეიძლება მოიცავდეს ეკონომიკურ, პოლიტიკურ ან სხვა ტიპის გარე ფაქტორებს, რომლებიც გავლენას ახდენენ ქვეყნის ეკონომიკაზე.\n","2. **ბიუჯეტის დეფიციტი**: ეს არის სიტუაცია, როდესაც მთავრობის ხარჯები აღემატება მის შემოსავლებს.\n","3. **2014 წლის ბიუჯეტი**: საჭიროა გადამოწმება, თუ რა მოხდა 2014 წელს საქართველოს ბიუჯეტთან დაკავშირებით.\n","\n","თუ ვიმსჯელებთ არსებული მონაცემების მიხედვით, 2014 წელს საქართველოს მთავრობამ მართლაც შეამცირა ბიუჯეტის დეფიციტი. ეს შეიძლება იყოს დაკავშირებული საგარეო შოკების შერბილების მცდელობასთან, თუმცა ეს არ არის პირდაპირი მტკიცებულება, რომ ეს იყო ერთადერთი ან მთავარი მიზეზი.\n","\n","საბოლოო პასუხი: „ნახევრად მართალია“.\n","Inconclusive response\n","Model Output: საბოლოო პასუხი: მცდარი\n","\n","საქართველოში უცხოელებს შეუძლიათ შეიძინონ მიწის ნაკვეთები, მაგრამ არსებობს გარკვეული შეზღუდვები და რეგულაციები. მაგალითად, სასოფლო-სამეურნეო მიწის შეძენა უცხოელებისთვის შეზღუდულია. ამიტომ, პრეტენზია, რომ \"ყველას [უცხოელს] შეუძლია შეიძინოს მიწის ნაკვეთები, ყველანაირი შეზღუდვის გარეშე\" არ არის სწორი.\n","Inconclusive response\n","Model Output: საბოლოო პასუხი: ნახევრად მართალია.\n","Inconclusive response\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 134\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompts:  \u001b[38;5;66;03m# Skip unsupported languages\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mget_gpt_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclaim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m cleaned_output \u001b[38;5;241m=\u001b[39m clean_output(output)\n","Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mget_gpt_response\u001b[0;34m(claim, language, model)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     user_prompt \u001b[38;5;241m=\u001b[39m prompts[language]\u001b[38;5;241m.\u001b[39mformat(claim\u001b[38;5;241m=\u001b[39mclaim)\n\u001b[0;32m---> 65\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n","File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import pandas as pd\n","import openai\n","import json\n","import os\n","import re\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Retrieve the OpenAI API key from the environment variable\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# Define the prompting method and language family\n","language_family = \"kartvelian\"\n","method = \"direct_inference\"\n","model = \"gpt-4o\"\n","\n","# Load data\n","input_file_path = f'../../JSON Files/{language_family}.json'\n","output_file_path = f\"{language_family}_{model}_{method}.csv\"\n","summary_file_path = f\"{language_family}_{model}_{method}_summary.json\"\n","\n","if os.path.exists(input_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","else:\n","    raise FileNotFoundError(f\"File not found: {input_file_path}\")\n","\n","claims = [item['claim'] for item in data]\n","labels = [item['label'] for item in data]\n","languages = [item['language'] for item in data]\n","sites = [item['site'] for item in data]\n","\n","df = pd.DataFrame({\n","    'claim': claims,\n","    'label': labels,\n","    'language': languages,\n","    'site': sites\n","})\n","\n","# Language-specific user prompts and final answer mappings\n","prompts = {\n","    \"ka\": \"'{claim}' არის ეს პრეტენზია «მართალია», «ძირითადად მართალია», «ნახევრად მართალია», «ძირითადად მცდარი» ან «მცდარი»? თქვენ ყოველთვის უნდა დარწმუნდეთ, რომ თქვენს საბოლოო პასუხს აქვს პრეფიქსი „საბოლოო პასუხი:“ რასაც მოჰყვება „მართალი“, „ძირითადად მართალია“, „ნახევრად მართალია“, „ძირითადად მცდარი“ ან „მცდარი“.\"\n","}\n","\n","final_answer_mappings = {\n","    \"ka\": {\n","        \"საბოლოო პასუხი: ჭეშმარიტი\": \"true\",\n","        \"საბოლოო პასუხი: უმეტესად ჭეშმარიტი\": \"mostly true\",\n","        \"საბოლოო პასუხი: ნახევრად ჭეშმარიტი\": \"half true\",\n","        \"საბოლოო პასუხი: უმეტესად ცრუ\": \"mostly false\",\n","        \"საბოლოო პასუხი: ცრუ\": \"false\",\n","        \"ჭეშმარიტი\": \"true\",\n","        \"უმეტესად ჭეშმარიტი\": \"mostly true\",\n","        \"ნახევრად ჭეშმარიტი\": \"half true\",\n","        \"უმეტესად ცრუ\": \"mostly false\",\n","        \"ცრუ\": \"false\"\n","    }\n","}\n","\n","def get_gpt_response(claim, language, model=model):\n","    try:\n","        user_prompt = prompts[language].format(claim=claim)\n","        response = openai.ChatCompletion.create(\n","            model=model,\n","            messages=[\n","                {\"role\": \"user\", \"content\": user_prompt}\n","            ],\n","            temperature=0,\n","        )\n","        return response['choices'][0]['message']['content']\n","    except Exception as e:\n","        print(f\"Error getting GPT response: {e}\")\n","        return \"\"\n","\n","def clean_output(output):\n","    cleaned_output = re.sub(r'[^a-zA-Zა-ჰ\\s:]', '', output)\n","    return cleaned_output.lower()\n","\n","def extract_final_answer(cleaned_output, language):\n","    try:\n","        final_answer = None\n","        # First, look for the exact prefix \"საბოლოო პასუხი:\"\n","        match = re.search(r'საბოლოო პასუხი[:]*\\s*([\\w\\s]+)', cleaned_output)\n","        if match:\n","            response = match.group(1).strip()\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in response:\n","                    final_answer = answer\n","                    break\n","        if not final_answer:\n","            # If no exact match found, look for keywords directly in the cleaned output\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in cleaned_output:\n","                    final_answer = answer\n","                    break\n","        return final_answer\n","    except Exception as e:\n","        print(f\"Error extracting final answer: {e}\")\n","    return None\n","\n","# Initialize or load existing outputs and summary\n","if os.path.exists(output_file_path):\n","    output_df = pd.read_csv(output_file_path)\n","    outputs = output_df.to_dict('records')\n","else:\n","    outputs = []\n","\n","if os.path.exists(summary_file_path):\n","    with open(summary_file_path, 'r', encoding='utf-8') as file:\n","        summary = json.load(file)\n","else:\n","    summary = {\n","        'correct': 0,\n","        'wrong': 0,\n","        'inconclusive': 0,\n","        'total': 0,\n","        'languages': {}\n","    }\n","\n","# Process claims and update files iteratively\n","for index, row in df.iterrows():\n","    if any(output['claim'] == row['claim'] for output in outputs):\n","        continue  # Skip already processed claims\n","\n","    claim = row['claim']\n","    label = row['label']\n","    language = row['language']\n","    \n","    if language not in prompts:  # Skip unsupported languages\n","        continue\n","\n","    output = get_gpt_response(claim, language)\n","    \n","    print(f\"Model Output: {output}\")\n","    \n","    cleaned_output = clean_output(output)\n","    final_answer = extract_final_answer(cleaned_output, language)\n","    \n","    if final_answer is None:\n","        print(\"Inconclusive response\")\n","        summary['inconclusive'] += 1\n","    else:\n","        print(f\"Final Answer: {final_answer.capitalize()}, Actual Answer: {label.capitalize()}\")\n","        if final_answer == label.lower():\n","            print(\"Correct response\")\n","            summary['correct'] += 1\n","        else:\n","            print(\"Wrong response\")\n","            summary['wrong'] += 1\n","    \n","    # Save outputs\n","    output_record = {\n","        'claim': claim,\n","        'label': label,\n","        'language': language,\n","        'output': output,\n","        'final_answer': final_answer,\n","        'correct': final_answer == label.lower() if final_answer else False,\n","        'inconclusive': final_answer is None\n","    }\n","    outputs.append(output_record)\n","    \n","    # Update language summary\n","    if language not in summary['languages']:\n","        summary['languages'][language] = {'correct': 0, 'wrong': 0, 'inconclusive': 0, 'total': 0}\n","    summary['languages'][language]['total'] += 1\n","    summary['total'] += 1\n","    if final_answer is None:\n","        summary['languages'][language]['inconclusive'] += 1\n","    elif final_answer == label.lower():\n","        summary['languages'][language]['correct'] += 1\n","    else:\n","        summary['languages'][language]['wrong'] += 1\n","\n","    # Save results to CSV iteratively\n","    pd.DataFrame(outputs).to_csv(output_file_path, index=False, encoding='utf-8')\n","\n","    # Save summary to JSON iteratively\n","    with open(summary_file_path, 'w', encoding='utf-8') as file:\n","        json.dump(summary, file, ensure_ascii=False, indent=4)\n","\n","print(f\"Results saved to {output_file_path} and {summary_file_path}\")\n","print(f\"Summary: {summary}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Slavic"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import openai\n","import json\n","import os\n","import re\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Retrieve the OpenAI API key from the environment variable\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# Define the prompting method and language family\n","language_family = \"slavic\"\n","method = \"direct_inference\"\n","model = \"gpt-4o\"\n","\n","# Load data\n","input_file_path = f'../../JSON Files/{language_family}.json'\n","output_file_path = f\"{language_family}_{model}_{method}.csv\"\n","summary_file_path = f\"{language_family}_{model}_{method}_summary.json\"\n","\n","if os.path.exists(input_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","else:\n","    raise FileNotFoundError(f\"File not found: {input_file_path}\")\n","\n","claims = [item['claim'] for item in data]\n","labels = [item['label'] for item in data]\n","languages = [item['language'] for item in data]\n","sites = [item['site'] for item in data]\n","\n","df = pd.DataFrame({\n","    'claim': claims,\n","    'label': labels,\n","    'language': languages,\n","    'site': sites\n","})\n","\n","# Language-specific user prompts and final answer mappings\n","prompts = {\n","    \"ru\": \"'{claim}' является ли это утверждение «правдивым», «в основном правдивым», «наполовину правдивым», «в основном ложным» или «ложным»? Вы должны всегда убедиться, что ваш окончательный ответ начинается с «Окончательный ответ:» и далее следует «Правда», «В основном правда», «Наполовину правда», «В основном ложь» или «Ложь».\",\n","    \"pl\": \"'{claim}' czy to stwierdzenie jest „prawdziwe”, „przeważnie prawdziwe”, „w połowie prawdziwe”, „przeważnie fałszywe” czy „fałszywe”? Musisz zawsze upewnić się, że twoja ostateczna odpowiedź zaczyna się od „Ostateczna odpowiedź:” a następnie „Prawda”, „Przeważnie prawda”, „W połowie prawda”, „Przeważnie fałsz” lub „Fałsz”.\",\n","    \"sr\": \"'{claim}' да ли је ова изјава „истинита”, „углавном истинита”, „полуистинита”, „углавном лажна” или „лажна”? Морате увек осигурати да ваш коначан одговор почиње са „Коначан одговор:” и затим следи „Истина”, „Углавном истина”, „Полуистина”, „Углавном лаж” или „Лаж”.\"\n","}\n","\n","final_answer_mappings = {\n","    \"ru\": {\n","        \"окончательный ответ: правда\": \"true\",\n","        \"окончательный ответ: в основном правда\": \"mostly true\",\n","        \"окончательный ответ: наполовину правда\": \"half true\",\n","        \"окончательный ответ: в основном ложь\": \"mostly false\",\n","        \"окончательный ответ: ложь\": \"false\",\n","        \"правда\": \"true\",\n","        \"в основном правда\": \"mostly true\",\n","        \"наполовину правда\": \"half true\",\n","        \"в основном ложь\": \"mostly false\",\n","        \"ложь\": \"false\"\n","    },\n","    \"pl\": {\n","        \"ostateczna odpowiedź: prawda\": \"true\",\n","        \"ostateczna odpowiedź: przeważnie prawda\": \"mostly true\",\n","        \"ostateczna odpowiedź: w połowie prawda\": \"half true\",\n","        \"ostateczna odpowiedź: przeważnie fałsz\": \"mostly false\",\n","        \"ostateczna odpowiedź: fałsz\": \"false\",\n","        \"prawda\": \"true\",\n","        \"przeważnie prawda\": \"mostly true\",\n","        \"w połowie prawda\": \"half true\",\n","        \"przeważnie fałsz\": \"mostly false\",\n","        \"fałsz\": \"false\"\n","    },\n","    \"sr\": {\n","        \"коначан одговор: истина\": \"true\",\n","        \"коначан одговор: углавном истина\": \"mostly true\",\n","        \"коначан одговор: полуистина\": \"half true\",\n","        \"коначан одговор: углавном лаж\": \"mostly false\",\n","        \"коначан одговор: лаж\": \"false\",\n","        \"истина\": \"true\",\n","        \"углавном истина\": \"mostly true\",\n","        \"полуистина\": \"half true\",\n","        \"углавном лаж\": \"mostly false\",\n","        \"лаж\": \"false\"\n","    }\n","}\n","\n","def get_gpt_response(claim, language, model=model):\n","    try:\n","        user_prompt = prompts[language].format(claim=claim)\n","        response = openai.ChatCompletion.create(\n","            model=model,\n","            messages=[\n","                {\"role\": \"user\", \"content\": user_prompt}\n","            ],\n","            temperature=0,\n","        )\n","        return response['choices'][0]['message']['content']\n","    except Exception as e:\n","        print(f\"Error getting GPT response: {e}\")\n","        return \"\"\n","\n","def clean_output(output):\n","    cleaned_output = re.sub(r'[^a-zA-Zа-яА-Я\\s:]', '', output)\n","    return cleaned_output.lower()\n","\n","def extract_final_answer(cleaned_output, language):\n","    try:\n","        final_answer = re.search(r'(окончательный ответ:|остateczna odpowiedź:|коначан одговор:)\\s*([\\w\\s]+)', cleaned_output)\n","        if final_answer:\n","            response = final_answer.group(2).strip()\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in response:\n","                    return answer\n","        else:\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in cleaned_output:\n","                    return answer\n","    except Exception as e:\n","        print(f\"Error extracting final answer: {e}\")\n","    return None\n","\n","# Initialize or load existing outputs and summary\n","if os.path.exists(output_file_path):\n","    output_df = pd.read_csv(output_file_path)\n","    outputs = output_df.to_dict('records')\n","else:\n","    outputs = []\n","\n","if os.path.exists(summary_file_path):\n","    with open(summary_file_path, 'r', encoding='utf-8') as file:\n","        summary = json.load(file)\n","else:\n","    summary = {\n","        'correct': 0,\n","        'wrong': 0,\n","        'inconclusive': 0,\n","        'total': 0,\n","        'languages': {}\n","    }\n","\n","# Process claims and update files iteratively\n","for index, row in df.iterrows():\n","    if any(output['claim'] == row['claim'] for output in outputs):\n","        continue  # Skip already processed claims\n","\n","    claim = row['claim']\n","    label = row['label']\n","    language = row['language']\n","    \n","    if language not in prompts:  # Skip unsupported languages\n","        continue\n","\n","    output = get_gpt_response(claim, language)\n","    \n","    print(f\"Model Output: {output}\")\n","    \n","    cleaned_output = clean_output(output)\n","    final_answer = extract_final_answer(cleaned_output, language)\n","    \n","    if final_answer is None:\n","        print(\"Inconclusive response\")\n","        summary['inconclusive'] += 1\n","    else:\n","        print(f\"Final Answer: {final_answer.capitalize()}, Actual Answer: {label.capitalize()}\")\n","        if final_answer == label.lower():\n","            print(\"Correct response\")\n","            summary['correct'] += 1\n","        else:\n","            print(\"Wrong response\")\n","            summary['wrong'] += 1\n","    \n","    # Save outputs\n","    output_record = {\n","        'claim': claim,\n","        'label': label,\n","        'language': language,\n","        'output': output,\n","        'final_answer': final_answer,\n","        'correct': final_answer == label.lower() if final_answer else False,\n","        'inconclusive': final_answer is None\n","    }\n","    outputs.append(output_record)\n","    \n","    # Update language summary\n","    if language not in summary['languages']:\n","        summary['languages'][language] = {'correct': 0, 'wrong': 0, 'inconclusive': 0, 'total': 0}\n","    summary['languages'][language]['total'] += 1\n","    summary['total'] += 1\n","    if final_answer is None:\n","        summary['languages'][language]['inconclusive'] += 1\n","    elif final_answer == label.lower():\n","        summary['languages'][language]['correct'] += 1\n","    else:\n","        summary['languages'][language]['wrong'] += 1\n","\n","    # Save results to CSV iteratively\n","    pd.DataFrame(outputs).to_csv(output_file_path, index=False, encoding='utf-8')\n","\n","    # Save summary to JSON iteratively\n","    with open(summary_file_path, 'w', encoding='utf-8') as file:\n","        json.dump(summary, file, ensure_ascii=False, indent=4)\n","\n","print(f\"Results saved to {output_file_path} and {summary_file_path}\")\n","print(f\"Summary: {summary}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Turkic"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import openai\n","import json\n","import os\n","import re\n","from dotenv import load_dotenv\n","\n","# Load environment variables from .env file\n","load_dotenv()\n","\n","# Retrieve the OpenAI API key from the environment variable\n","openai.api_key = os.getenv('OPENAI_API_KEY')\n","\n","# Define the prompting method and language family\n","language_family = \"turkic\"\n","method = \"direct_inference\"\n","model = \"gpt-4o\"\n","\n","# Load data\n","input_file_path = f'../../JSON Files/{language_family}.json'\n","output_file_path = f\"{language_family}_{model}_{method}.csv\"\n","summary_file_path = f\"{language_family}_{model}_{method}_summary.json\"\n","\n","if os.path.exists(input_file_path):\n","    with open(input_file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","else:\n","    raise FileNotFoundError(f\"File not found: {input_file_path}\")\n","\n","claims = [item['claim'] for item in data]\n","labels = [item['label'] for item in data]\n","languages = [item['language'] for item in data]\n","sites = [item['site'] for item in data]\n","\n","df = pd.DataFrame({\n","    'claim': claims,\n","    'label': labels,\n","    'language': languages,\n","    'site': sites\n","})\n","\n","# Language-specific user prompts and final answer mappings\n","prompts = {\n","    \"tr\": \"'{claim}' bu ifade «doğru», «çoğunlukla doğru», «yarı doğru», «çoğunlukla yanlış» veya «yanlış» mı? «Nihai Cevap: Doğru», «Nihai Cevap: Çoğunlukla Doğru», «Nihai Cevap: Yarı Doğru», «Nihai Cevap: Çoğunlukla Yanlış» veya «Nihai Cevap: Yanlış» şeklinde cevap verin.\",\n","    \"az\": \"'{claim}' bu ifadə «doğru», «əsasən doğru», «yarı doğru», «əsasən yanlış» və ya «yanlış»dır? «Son Cavab: Doğru», «Son Cavab: Əsasən Doğru», «Son Cavab: Yarı Doğru», «Son Cavab: Əsasən Yanlış» və ya «Son Cavab: Yanlış» olaraq cavab verin.\"\n","}\n","\n","final_answer_mappings = {\n","    \"tr\": {\n","        \"nihai cevap: doğru\": \"true\",\n","        \"nihai cevap: çoğunlukla doğru\": \"mostly true\",\n","        \"nihai cevap: yarı doğru\": \"half true\",\n","        \"nihai cevap: çoğunlukla yanlış\": \"mostly false\",\n","        \"nihai cevap: yanlış\": \"false\",\n","        \"doğru\": \"true\",\n","        \"çoğunlukla doğru\": \"mostly true\",\n","        \"yarı doğru\": \"half true\",\n","        \"çoğunlukla yanlış\": \"mostly false\",\n","        \"yanlış\": \"false\"\n","    },\n","    \"az\": {\n","        \"son cavab: doğru\": \"true\",\n","        \"son cavab: əsasən doğru\": \"mostly true\",\n","        \"son cavab: yarı doğru\": \"half true\",\n","        \"son cavab: əsasən yanlış\": \"mostly false\",\n","        \"son cavab: yanlış\": \"false\",\n","        \"doğru\": \"true\",\n","        \"əsasən doğru\": \"mostly true\",\n","        \"yarı doğru\": \"half true\",\n","        \"əsasən yanlış\": \"mostly false\",\n","        \"yanlış\": \"false\"\n","    }\n","}\n","\n","def get_gpt_response(claim, language, model=model):\n","    try:\n","        user_prompt = prompts[language].format(claim=claim)\n","        response = openai.ChatCompletion.create(\n","            model=model,\n","            messages=[\n","                {\"role\": \"user\", \"content\": user_prompt}\n","            ],\n","            temperature=0,\n","        )\n","        return response['choices'][0]['message']['content']\n","    except Exception as e:\n","        print(f\"Error getting GPT response: {e}\")\n","        return \"\"\n","\n","def clean_output(output):\n","    cleaned_output = re.sub(r'[^a-zA-ZəƏıİşŞöÖüÜğĞçÇ\\s:]', '', output)\n","    return cleaned_output.lower()\n","\n","def extract_final_answer(cleaned_output, language):\n","    try:\n","        final_answer = re.search(r'(nihai cevap:|son cavab:)\\s*([\\w\\s]+)', cleaned_output)\n","        if final_answer:\n","            response = final_answer.group(2).strip()\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in response:\n","                    return answer\n","        else:\n","            for keyword, answer in final_answer_mappings[language].items():\n","                if keyword in cleaned_output:\n","                    return answer\n","    except Exception as e:\n","        print(f\"Error extracting final answer: {e}\")\n","    return None\n","\n","# Initialize or load existing outputs and summary\n","if os.path.exists(output_file_path):\n","    output_df = pd.read_csv(output_file_path)\n","    outputs = output_df.to_dict('records')\n","else:\n","    outputs = []\n","\n","if os.path.exists(summary_file_path):\n","    with open(summary_file_path, 'r', encoding='utf-8') as file:\n","        summary = json.load(file)\n","else:\n","    summary = {\n","        'correct': 0,\n","        'wrong': 0,\n","        'inconclusive': 0,\n","        'total': 0,\n","        'languages': {}\n","    }\n","\n","# Process claims and update files iteratively\n","for index, row in df.iterrows():\n","    if any(output['claim'] == row['claim'] for output in outputs):\n","        continue  # Skip already processed claims\n","\n","    claim = row['claim']\n","    label = row['label']\n","    language = row['language']\n","    \n","    if language not in prompts:  # Skip unsupported languages\n","        continue\n","\n","    output = get_gpt_response(claim, language)\n","    \n","    print(f\"Model Output: {output}\")\n","    \n","    cleaned_output = clean_output(output)\n","    final_answer = extract_final_answer(cleaned_output, language)\n","    \n","    if final_answer is None:\n","        print(\"Inconclusive response\")\n","        summary['inconclusive'] += 1\n","    else:\n","        print(f\"Final Answer: {final_answer.capitalize()}, Actual Answer: {label.capitalize()}\")\n","        if final_answer == label.lower():\n","            print(\"Correct response\")\n","            summary['correct'] += 1\n","        else:\n","            print(\"Wrong response\")\n","            summary['wrong'] += 1\n","    \n","    # Save outputs\n","    output_record = {\n","        'claim': claim,\n","        'label': label,\n","        'language': language,\n","        'output': output,\n","        'final_answer': final_answer,\n","        'correct': final_answer == label.lower() if final_answer else False,\n","        'inconclusive': final_answer is None\n","    }\n","    outputs.append(output_record)\n","    \n","    # Update language summary\n","    if language not in summary['languages']:\n","        summary['languages'][language] = {'correct': 0, 'wrong': 0, 'inconclusive': 0, 'total': 0}\n","    summary['languages'][language]['total'] += 1\n","    summary['total'] += 1\n","    if final_answer is None:\n","        summary['languages'][language]['inconclusive'] += 1\n","    elif final_answer == label.lower():\n","        summary['languages'][language]['correct'] += 1\n","    else:\n","        summary['languages'][language]['wrong'] += 1\n","\n","    # Save results to CSV iteratively\n","    pd.DataFrame(outputs).to_csv(output_file_path, index=False, encoding='utf-8')\n","\n","    # Save summary to JSON iteratively\n","    with open(summary_file_path, 'w', encoding='utf-8') as file:\n","        json.dump(summary, file, ensure_ascii=False, indent=4)\n","\n","print(f\"Results saved to {output_file_path} and {summary_file_path}\")\n","print(f\"Summary: {summary}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"asdrp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
